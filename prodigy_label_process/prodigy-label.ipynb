{"cells":[{"cell_type":"markdown","metadata":{},"source":["# âš ï¸ WE NEED TO PRODIGY ðŸ¦„ TO FIX THE ANNOTATION\n","For example, `201210379231` is not a CHEMICAL"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# download prodigy for your system: https://gerdes.fr/saclay/informationRetrieval/prodigy/\n","# put the files in a subfolder\n","# try what works. for me it's:\n","# !pip install ./mac/prodigy-1.11.11-cp310-cp310-macosx_11_0_arm64.whl"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Prepare the patent text in prodigy format"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import json\n","\n","with open('C07K.txt', 'r') as f:\n","    texts = f.read().strip()\n","# split text in smaller chunks, then put in chunk in a list of dicts for prodigy\n","texts = texts.split('\\n')\n","texts = [text.strip() for text in texts]\n","\n","size = 1500\n","new_texts = []\n","for text in texts:\n","    #if text is too long, split it\n","    if len(text) > size:\n","        #split text in chunks\n","        chunks = [text[i:i+size] for i in range(0, len(text), size)]\n","        #add chunks to list\n","        new_texts.extend(chunks)\n","    elif len(text) > 0:\n","        new_texts.append(text)\n","\n","with open('C07K.jsonl', 'w') as outfile:\n","    for i in range(len(new_texts)):\n","        if i < 300000:\n","            continue\n","        json.dump({'text': new_texts[i]}, outfile)\n","        outfile.write('\\n')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Fix annotations with prodigy + ktgiahieu/bert-for-patents-finetuned-ner"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Added dataset ner_C07K_last2 to database SQLite.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","\n","âœ¨  Starting the web server at http://localhost:8080 ...\n","Open the app in your browser and start annotating!\n","\n","\u001b[38;5;3mâš  Front End Log - 2023-04-23 10:55:01+00:00: Duplicate _task_hash found\n","in Frontend batch.\u001b[0m\n","^C\n","\n","\u001b[38;5;2mâœ” Saved 266 annotations to database SQLite\u001b[0m\n","Dataset: ner_C07K_last2\n","Session ID: 2023-04-23_12-42-59\n","\n"]}],"source":["!prodigy bert.ner.manual ner_combine ./C07K.jsonl --hide-wp-prefix -F ner_bert_patent_manualv2.py"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Output annotations in Prodigy format (for SpaCy)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[38;5;2mâœ” Created dataset 'ner_combine' in database SQLite\u001b[0m\n","\u001b[38;5;2mâœ” Imported 777 annotations to 'ner_combine' (session\n","2023-04-23_13-30-30) in database SQLite\u001b[0m\n","Found and keeping existing \"answer\" in 777 examples\n"]}],"source":["!prodigy db-out ner_combine ner_combine.jsonl"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Output annotations in Huggingface format"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["import json\n","import spacy\n","\n","from prodigy.components.db import connect\n","\n","db = connect()\n","prodigy_annotations = db.get_dataset(\"ner_combine_filtered\")\n","examples = ((eg[\"text\"], eg) for eg in prodigy_annotations)\n","nlp = spacy.blank(\"en\")\n","\n","dataset = []\n","\n","for doc, eg in nlp.pipe(examples, as_tuples=True):\n","    if eg['answer']=='ignore':\n","        continue\n","    try:\n","        doc.ents = [doc.char_span(s[\"start\"], s[\"end\"], s[\"label\"]) for s in eg[\"spans\"]]\n","        iob_tags = [f\"{t.ent_iob_}-{t.ent_type_}\" if t.ent_iob_ else \"O\" for t in doc]\n","        iob_tags = [t.strip(\"-\") for t in iob_tags]\n","        tokens = [str(t) for t in doc]\n","        temp_data = {\n","            \"tokens\": tokens,\n","            \"tags\": iob_tags\n","        }\n","        dataset.append(temp_data)\n","    except:\n","        pass\n","\n","with open('data.jsonl', 'w') as outfile:\n","    for entry in dataset:\n","        if any([x!='O' for x in entry['tags']]):\n","            json.dump(entry, outfile)\n","            outfile.write('\\n')\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Finally, let's fine-tune the model again using these new annotations:\n","- Upload `data.jsonl` to Huggingface Hub\n","- (You can use our annotated data directly `https://huggingface.co/datasets/ktgiahieu/IR_combined_filtered`)\n","- Fine-tune another model with the finetune notebook ðŸ““: https://colab.research.google.com/drive/1OzCY782KJSF0FBDS0d1CoMhfp3-RtJMV"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
